{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmitryplus/otus_llm_develop_dz/blob/main/hw1_attention_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –î–ó ‚Ññ1: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ Attention –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö\n",
        "\n",
        "## üéØ –¶–µ–ª—å –∑–∞–¥–∞–Ω–∏—è\n",
        "–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞–Ω–∏—è –≤—ã —Å–º–æ–∂–µ—Ç–µ:\n",
        "- –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É self-attention –º–µ—Ö–∞–Ω–∏–∑–º–∞\n",
        "- –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ QKV –≤ BERT, GPT –∏ T5\n",
        "- –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –∏ –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å attention-–∫–∞—Ä—Ç—ã –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π –º–æ–¥–µ–ª–∏\n",
        "\n",
        "## üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–¥–∞–Ω–∏—è\n",
        "- **–ß–∞—Å—Ç—å 1** (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è, 70% –æ—Ü–µ–Ω–∫–∏): –ë–∞–∑–æ–≤–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention\n",
        "- **–ß–∞—Å—Ç—å 2** (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è, 30% –æ—Ü–µ–Ω–∫–∏): –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "\n",
        "## ‚ö° –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏\n",
        "- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–¥–∞: 40%\n",
        "- –ö–∞—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: 30%\n",
        "- –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: 30%\n"
      ],
      "metadata": {
        "id": "MxDqF_bldL05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "\n",
        "–£—Å—Ç–∞–Ω–æ–≤–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.\n"
      ],
      "metadata": {
        "id": "1E9aKTyCdSqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcc95BIlXZQY"
      },
      "outputs": [],
      "source": [
        "%pip install transformers torch matplotlib seaborn bertviz numpy pandas\n",
        "%pip install -q ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riGO2qpnXZQZ"
      },
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel,\n",
        "    BertTokenizer, BertModel,\n",
        "    GPT2Tokenizer, GPT2Model,\n",
        "    T5Tokenizer, T5Model\n",
        ")\n",
        "from bertviz import model_view, head_view\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä –ß–∞—Å—Ç—å 1: –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å)\n",
        "\n",
        "### –ó–∞–¥–∞–Ω–∏–µ 1.1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π\n"
      ],
      "metadata": {
        "id": "jBKZfirbeNyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7wv662LXZQZ"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "models_info = {\n",
        "    'BERT': {\n",
        "        'model_name': 'bert-base-uncased',\n",
        "        'tokenizer': None,\n",
        "        'model': None\n",
        "    },\n",
        "    'GPT-2': {\n",
        "        'model_name': 'gpt2',\n",
        "        'tokenizer': None,\n",
        "        'model': None\n",
        "    }\n",
        "}\n",
        "\n",
        "# TODO: –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –∏ –º–æ–¥–µ–ª–∏ –¥–ª—è BERT –∏ GPT-2\n",
        "# –∏—Å–ø–æ–ª—å–∑—É—è –±–∏–±–ª–∏–æ—Ç–µ–∫—É Transformers\n",
        "#\n",
        "# –ó–∞–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–æ–≤–∞—Ä—å models_info\n",
        "\n",
        "models_info['BERT']['tokenizer'] = ...\n",
        "models_info['BERT']['model'] = ...\n",
        "models_info['GPT-2']['tokenizer'] = ...\n",
        "models_info['GPT-2']['model'] = ...\n",
        "\n",
        "print(\"–ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 1.2: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ attention –≤–µ—Å–æ–≤"
      ],
      "metadata": {
        "id": "-W7CSwzndYAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTKNeGW6XZQZ"
      },
      "outputs": [],
      "source": [
        "# –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "test_text = \"The cat sat on the mat and looked at the dog.\"\n",
        "print(f\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π —Ç–µ–∫—Å—Ç: {test_text}\")\n",
        "\n",
        "def get_attention_weights(text, model_name):\n",
        "    \"\"\"\n",
        "    –ü–æ–ª—É—á–µ–Ω–∏–µ attention –≤–µ—Å–æ–≤ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –º–æ–¥–µ–ª–∏\n",
        "    \"\"\"\n",
        "    tokenizer = models_info[model_name]['tokenizer']\n",
        "    model = models_info[model_name]['model']\n",
        "\n",
        "    # ......\n",
        "\n",
        "    return attention_weights, tokens, inputs\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º attention –≤–µ—Å–∞ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
        "bert_attention, bert_tokens, bert_inputs = get_attention_weights(test_text, 'BERT')\n",
        "gpt2_attention, gpt2_tokens, gpt2_inputs = get_attention_weights(test_text, 'GPT-2')\n",
        "\n",
        "print(f\"BERT tokens: {bert_tokens}\")\n",
        "print(f\"GPT-2 tokens: {gpt2_tokens}\")\n",
        "print(f\"BERT attention shape: {bert_attention[0].shape}\")\n",
        "print(f\"GPT-2 attention shape: {gpt2_attention[0].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 1.3: –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π attention heatmap"
      ],
      "metadata": {
        "id": "GjEP2rS6dc82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWBKZMT7XZQZ"
      },
      "outputs": [],
      "source": [
        "def plot_attention_heatmap(attention_weights, tokens, model_name, layer_idx=0, head_idx=0):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–Ω–∏–µ heatmap –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ attention –≤–µ—Å–æ–≤\n",
        "    \"\"\"\n",
        "    # TODO: –ò–∑–≤–ª–µ–∫–∏—Ç–µ attention –≤–µ—Å–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–ª–æ—è –∏ –≥–æ–ª–æ–≤—ã\n",
        "    attention_matrix = ...\n",
        "\n",
        "    # ToDo: –°–æ–∑–¥–∞–µ–º heatmap\n",
        "\n",
        "\n",
        "    # TODO: –°–æ–∑–¥–∞–Ω–∏–µ–µ heatmap –∏—Å–ø–æ–ª—å–∑—É—è seaborn ( –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏)\n",
        "    # –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω ( —Å–ª–æ–≤–æ )\n",
        "    # plt.figure(figsize=(10, 8))\n",
        "    # sns.heatmap(\n",
        "    #    attention_matrix,\n",
        "    #    xticklabels=tokens,\n",
        "    #    yticklabels=tokens,\n",
        "    #    cmap='Blues',\n",
        "    #    cbar=True,\n",
        "    #    square=True\n",
        "    # )\n",
        "\n",
        "    # plt.title(f'{model_name} - Layer {layer_idx}, Head {head_idx}\\\\nAttention Weights')\n",
        "    # plt.xlabel('Key (Attended to)')\n",
        "    # plt.ylabel('Query (Attending from)')\n",
        "    # plt.xticks(rotation=45)\n",
        "    # plt.yticks(rotation=0)\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º attention –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –∏ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã\n",
        "plot_attention_heatmap(bert_attention, bert_tokens, 'BERT', layer_idx=0, head_idx=0)\n",
        "plot_attention_heatmap(gpt2_attention, gpt2_tokens, 'GPT-2', layer_idx=0, head_idx=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 1.4: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "\n",
        "**TODO: –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:**\n"
      ],
      "metadata": {
        "id": "lB3bsy80divi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **–ö–∞–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è –≤—ã –∑–∞–º–µ—Ç–∏–ª–∏ –≤ attention patterns –º–µ–∂–¥—É BERT –∏ GPT-2?**\n",
        "\n",
        "   *–í–∞—à –æ—Ç–≤–µ—Ç:*\n",
        "\n",
        "2. **–ö–∞–∫ –∏–∑–º–µ–Ω—è—é—Ç—Å—è attention –≤–µ—Å–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ—è—Ö?**\n",
        "\n",
        "   *–í–∞—à –æ—Ç–≤–µ—Ç:*\n",
        "\n",
        "3. **–ö–∞–∫—É—é —Ä–æ–ª—å –∏–≥—Ä–∞—é—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏?**\n",
        "\n",
        "   *–í–∞—à –æ—Ç–≤–µ—Ç:*\n"
      ],
      "metadata": {
        "id": "Yy3xITT8dlLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ –ß–∞—Å—Ç—å 2: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è (30% –æ—Ü–µ–Ω–∫–∏)\n",
        "\n",
        "### –ó–∞–¥–∞–Ω–∏–µ 2.1: –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Multi-Head Attention\n"
      ],
      "metadata": {
        "id": "fRTHuMq4drCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glYob05iXZQa"
      },
      "outputs": [],
      "source": [
        "def analyze_multi_head_attention(attention_weights, tokens, model_name, layer_idx=6):\n",
        "    \"\"\"\n",
        "    –ê–Ω–∞–ª–∏–∑ —Ç–æ–≥–æ, –∫–∞–∫ —Ä–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã attention —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö\n",
        "    \"\"\"\n",
        "    num_heads = ...\n",
        "\n",
        "    # TODO: –°–æ–∑–¥–∞–π—Ç–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é attention –¥–ª—è –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤ –æ–¥–Ω–æ–º —Å–ª–æ–µ\n",
        "\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º multi-head attention –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö —Å–ª–æ–µ–≤\n",
        "analyze_multi_head_attention(bert_attention, bert_tokens, 'BERT', layer_idx=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 2.2: –°–æ–∑–¥–∞–Ω–∏–µ attention-–∫–∞—Ä—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n"
      ],
      "metadata": {
        "id": "jO-KRUnrdwt5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH5iglp7XZQa"
      },
      "outputs": [],
      "source": [
        "# TODO: –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ attention –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
        "# ========= –†–ï–®–ï–ù–ò–ï: =========\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "# –ù–∞–ø—Ä–º–∏–º–µ—Ä –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DeepPavlov/rubert-base-cased\n",
        "ru_tokenizer = ...\n",
        "ru_model = ...\n",
        "\n",
        "# –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "russian_text = \"–ö–æ—Ç —Å–∏–¥–µ–ª –Ω–∞ –∫–æ–≤—Ä–µ –∏ —Å–º–æ—Ç—Ä–µ–ª –Ω–∞ —Å–æ–±–∞–∫—É.\"\n",
        "\n",
        "def analyze_russian_attention(text, tokenizer, model):\n",
        "    \"\"\"\n",
        "    –ê–Ω–∞–ª–∏–∑ attention –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
        "    \"\"\"\n",
        "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "    inputs = ...\n",
        "    outputs = ...\n",
        "    attention_weights = ...\n",
        "    tokens = ...\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º heatmap –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è\n",
        "    attention_matrix = ...\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(\n",
        "        attention_matrix,\n",
        "        xticklabels=tokens,\n",
        "        yticklabels=tokens,\n",
        "        cmap='Blues',\n",
        "        cbar=True,\n",
        "        square=True,\n",
        "        annot=True,\n",
        "        fmt='.2f'\n",
        "    )\n",
        "\n",
        "    plt.title(f'RuBERT Attention Heatmap\\\\nText: {text}')\n",
        "    plt.xlabel('Key (Attended to)')\n",
        "    plt.ylabel('Query (Attending from)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return attention_weights, tokens\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç\n",
        "ru_attention, ru_tokens = analyze_russian_attention(russian_text, ru_tokenizer, ru_model)\n",
        "\n",
        "print(f\"\\\\n–¢–æ–∫–µ–Ω—ã: {ru_tokens}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {len(ru_attention)}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤: {ru_attention[0].shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà –ò—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã–≤–æ–¥—ã\n",
        "\n",
        "### –ó–∞–¥–∞–Ω–∏–µ 3: –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "\n",
        "**TODO: –°–¥–µ–ª–∞–π—Ç–µ –∏—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:**\n"
      ],
      "metadata": {
        "id": "BrRiYPM5d1y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É BERT –∏ GPT-2:**\n",
        "\n",
        "   *–í–∞—à –∞–Ω–∞–ª–∏–∑:*\n",
        "\n",
        "2. **–ö–∞–∫ attention –≥–æ–ª–æ–≤—ã —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è:**\n",
        "\n",
        "   *–í–∞—à –∞–Ω–∞–ª–∏–∑:*\n",
        "\n",
        "3. **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:**\n",
        "\n",
        "   *–í–∞—à –∞–Ω–∞–ª–∏–∑:*\n",
        "\n",
        "4. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π:**\n",
        "\n",
        "   *–í–∞—à –∞–Ω–∞–ª–∏–∑:*\n"
      ],
      "metadata": {
        "id": "CwEd_Gjid4Lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
        "\n",
        "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –≤—ã:\n",
        "- ‚úÖ –ò–∑—É—á–∏–ª–∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã attention –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö\n",
        "- ‚úÖ –°—Ä–∞–≤–Ω–∏–ª–∏ —Ä–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (BERT vs GPT-2)\n",
        "- ‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é attention –≥–æ–ª–æ–≤\n",
        "- ‚úÖ –†–∞–±–æ—Ç–∞–ª–∏ —Å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏\n",
        "\n",
        "–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ attention –±—É–¥—É—Ç –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è:\n",
        "- –û—Ç–ª–∞–¥–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π\n",
        "- –û–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π AI-—Å–∏—Å—Ç–µ–º\n",
        "- –í—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è –∑–∞–¥–∞—á\n",
        "- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "## üìã –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Å–¥–∞—á–∏ –î–ó\n",
        "\n",
        "### –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å (70% –æ—Ü–µ–Ω–∫–∏):\n",
        "- [x] –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –º–æ–¥–µ–ª–∏ BERT –∏ GPT-2\n",
        "- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è attention –≤–µ—Å–æ–≤\n",
        "- [x] –°–æ–∑–¥–∞–Ω–∞ –±–∞–∑–æ–≤–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention heatmap\n",
        "- [x] –î–∞–Ω—ã –æ—Ç–≤–µ—Ç—ã –Ω–∞ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã\n",
        "\n",
        "### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å (30% –æ—Ü–µ–Ω–∫–∏):\n",
        "- [x] –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã multi-head attention –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "- [x] –†–∞–±–æ—Ç–∞ —Å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª—å—é\n",
        "- [x] –ò—Ç–æ–≥–æ–≤—ã–π —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "\n",
        "**–£–¥–∞—á–∏ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –∏–∑—É—á–µ–Ω–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤! üöÄ**\n"
      ],
      "metadata": {
        "id": "OGX1jUBxd9uc"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}